from airflow.decorators import dag, task
from airflow.models.param import Param
from airflow.operators.python import get_current_context
from airflow.utils.dates import days_ago
from kubernetes.client import models as k8s


class Constants:
    DAG_NAME = "kg_data_pipeline"
    DAG_SCHEDULE = "0 15 * * *"  # 3 PM UTC or 8 AM PST - recurring schedule daily
    DAG_OWNER = "airflow"
    ORG = "ybor"  # {{ org-name }} - best generated by an archetype macro
    VENTURE = "playground"  # {{ venture-name }} - best generated by an archetype macro
    ENV = "dev"  # usually dev, stg or prod

    # CI/CD Configurations
    DOCKER_IMAGE_PREFIX = f"p6m.jfrog.io/{ORG}-{VENTURE}-docker/applications"
    KG_DRIVER_DOCKER_IMAGE = f"{DOCKER_IMAGE_PREFIX}/ybor-graph-query-adapter-server:sha-84ff421"


    PULL_SECRET = "dockerconfig"  # "regcred"
    KG_SECRETS = "kg-secrets"
    KG_CONFIGMAP = "kg-cmap"

    # other constants
    ISTIO_ANNOTATION = "sidecar.istio.io/inject"
    DO_NOT_EVICT = "karpenter.sh/do-not-evict"
    DO_NOT_CONSOLIDATE = "karpenter.sh/do-not-consolidate"
    DO_NOT_DISRUPT = "karpenter.sh/do-not-disrupt"

dag_parameters = {
    "graph_space": Param(
        default=None,
        type="string",
        title="Name of the graph space to be created",
        description="(mandatory) Graph space name is required to run the DAG"
    ),
    "csv_name": Param(
        default=None,
        type="string",
        title="Name of the csv file to be processed",
        description="(mandatory) CSV file name is required to run the DAG"
    ),
    "container_name": Param(
        default=None,
        type="string",
        title="Name of the container where csv file gets uploaded",
        description="(mandatory) Container name is needed to locate the csv"
    ),
    "storage_container_prefix": Param(
        default=None,
        type="string",
        title="Name of the folder inside container where csv gets listed",
        description="(mandatory) Folder name where csv listing happens"
    )
}

kg_configmap = k8s.V1EnvFromSource(
    config_map_ref=k8s.V1ConfigMapEnvSource(name=Constants.KG_CONFIGMAP)
)
kg_secrets = k8s.V1EnvFromSource(
        secret_ref=k8s.V1SecretEnvSource(name=Constants.KG_SECRETS)
    )

@task
def init_task(args) -> dict:
    from datetime import datetime

    print(f"init_task args = {args}")
    dag_run = get_current_context()["dag_run"]
    print(f"init_task dag_run = {dag_run}")

    return args

@task.kubernetes(
    task_id="kg_ingestion",
    name="kg_ingestion_task",
    namespace="airflow",
    image=Constants.KG_DRIVER_DOCKER_IMAGE,
    image_pull_policy="Always",
    in_cluster=True,
    get_logs=True,
    # service_account_name=Constants.TRANSFORMS_SERVICE_ACCOUNT,
    do_xcom_push=True,
    image_pull_secrets=[k8s.V1LocalObjectReference(Constants.PULL_SECRET)],
    is_delete_operator_pod=False,
    env_from=[kg_secrets, kg_configmap],
    # labels={"app": "transformations", "app_type": "driver"},
    # annotations={
    #     Constants.ISTIO_ANNOTATION: "false", Constants.DO_NOT_EVICT: "true",
    #     Constants.DO_NOT_CONSOLIDATE: "true", Constants.DO_NOT_DISRUPT: "true"},
    container_resources=k8s.V1ResourceRequirements(
        requests={"memory": "1Gi", "cpu": "2.0", "ephemeral-storage": "1Gi"},
        limits={"memory": "2Gi", "cpu": "2.0", "ephemeral-storage": "3Gi"},
    ),
    # priority_class_name="workflow",
)
def kg_ingestion_task(args: dict) -> dict:
    import logging

    # use {{ org_name }}_{{ venture_name }} archetype macro
    from ybor_graph_query_adapter.main import run

    logger = logging.getLogger(__name__)
    logging.basicConfig(level=logging.INFO)

    logger.info(f"input = {args}")
    result = run(graph_space=args["graph_space"], csv_name=args["csv_name"], container_name=args["container_name"], storage_container_prefix=args["storage_container_prefix"])
    logger.info(f"output = {result}")
    return args



@dag(
    dag_id=Constants.DAG_NAME,
    default_args={
        "owner": Constants.DAG_OWNER,
        "depends_on_past": False,
    },
    params=dag_parameters,
    start_date=days_ago(1),
    schedule_interval=None, # Constants.DAG_SCHEDULE,
    max_active_runs=1,
)
def workflow():
    # Note : This task will go away shortly
    @task
    def echo_task(args: dict) -> dict:
        print(args)
        return args

    # Scheduler Args
    scheduler_info = {
        "scheduler_from_date": "{{ prev_data_interval_end_success }}",
        "scheduler_to_date": "{{ data_interval_end }}",
    }
    # General Args
    args = {
        **scheduler_info,
        "graph_space": "{{ params.graph_space }}",
        "csv_name": "{{ params.csv_name }}",
        "container_name": "{{ params.container_name }}",
        "storage_container_prefix": "{{ params.storage_container_prefix }}"
    }
    
    # pipe output of each stage as input to the next stage
    init_output = init_task(args)
    kg_output = kg_ingestion_task(init_output)
    echo_task(kg_output)


workflow()
